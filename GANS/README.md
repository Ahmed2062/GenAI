This folder contains implementations of two Generative Adversarial Networks (GAN) architectures—Vanilla GAN and Deep Convolutional GAN (DCGAN) built to generate realistic handwritten digits using the MNIST dataset. The DCGAN architecture is designed without any dense layers, relying entirely on convolutional and transposed convolutional layers, making it spatially aware and fully convolutional from end to end. In contrast, the Vanilla GAN follows a more classic architecture using only fully connected dense layers. Both models use the Adam optimizer and binary cross-entropy loss. The training process includes per-epoch loss tracking and fixed-noise image generation for visualizing progress. Code optimizations like `@tf.function` is applied for faster execution. This setup serves as a hands-on exploration of GAN behavior and performance in image synthesis tasks.
A loss vs. epoch graph has also been plotted for the GAN models. However, it's important to note that in GANs, losses tend to be unstable and are not reliable indicators of model quality. 
Instead FID (Fréchet Inception Distance) which is a popular metric for evaluating the quality of images generated by a GAN (or any other generative model) is used.
## References
- [DCGAN: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434)
