This folder contains implementations of two Generative Adversarial Networks (GAN) architectures—Fully Connected GAN (FCGAN) and Deep Convolutional GAN (DCGAN) built to generate realistic handwritten digits using the MNIST dataset. The DCGAN architecture is designed without any dense layers, relying entirely on convolutional and transposed convolutional layers, making it spatially aware and fully convolutional from end to end. In contrast, the FCGAN follows a more classic architecture using only fully connected dense layers. Both models use the Adam optimizer (with β₁ = 0.5), binary cross-entropy loss, and include techniques like label smoothing for stabilizing discriminator training. The training process includes per-epoch loss tracking and fixed-noise image generation for visualizing progress. Code optimizations like `@tf.function` and `prefetch(tf.data.AUTOTUNE)` are applied for faster execution. This setup serves as a hands-on exploration of GAN behavior and performance in image synthesis tasks.
A loss vs. epoch graph has also been plotted for both FCGAN and DCGAN models. However, it's important to note that in GANs, losses tend to be unstable and are not reliable indicators of model quality. 
Instead FID (Fréchet Inception Distance) which is a popular metric for evaluating the quality of images generated by a GAN (or any other generative model) is used.
