This project features a fine-tuned GPT-2 model trained on a custom dataset of over 15,000 poems scraped from poets.org. The dataset was cleaned and formatted for use with Hugging Face Transformers, with the goal of exploring creative text generation through poetic structure. While the results vary and may fall short of truly expressive or high-quality poetry, the project highlights both the potential and challenges of training generative models on web-scraped creative writing. Limitations in dataset consistency and literary quality likely impacted the final outputs, but the model still serves as a useful experimental baseline for further fine-tuning or prompt engineering in creative AI work.
